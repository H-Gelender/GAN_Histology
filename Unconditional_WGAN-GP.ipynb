{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "#Common libraries\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os \n",
    "import cv2\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = 'cuda'\n",
    "else:\n",
    "  device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENSLIDE_PATH = r'your/openslide/bin/repository'\n",
    "\n",
    "if hasattr(os, 'add_dll_directory'):\n",
    "    # Windows\n",
    "    with os.add_dll_directory(OPENSLIDE_PATH):\n",
    "        import openslide\n",
    "else:\n",
    "    import openslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_folder(image: None, threshold=0.1)->bool:\n",
    "\n",
    "    image = np.array(image)\n",
    "    image = image.astype(np.float32)\n",
    "    image/=255.\n",
    "\n",
    "    refrence = np.ones_like(image, dtype = np.float32)*250\n",
    "    refrence/=255.\n",
    "    diff = np.abs(image - refrence)\n",
    "    average_diff = np.mean(diff)\n",
    "    \n",
    "    return average_diff > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your Dataset with openslide.\n",
    "\n",
    "Pyramidal images are a commonly used technique in histopathology to represent images at different resolutions. They are constructed by dividing a high-resolution image into a series of sub-images at decreasing resolutions, organized in a pyramid shape. Each level of the pyramid represents a version of the original image at a different resolution, ranging from very high resolution to very low resolution.\n",
    "\n",
    "**Utility of Pyramidal Images in Histopathology**\n",
    "\n",
    "Pyramidal images are extremely useful in histopathology for several reasons:\n",
    "\n",
    "**Multi-scale Analysis:** Pathologists can examine fine details of tissues at high resolution while having an overview of the tissue at lower resolution. This allows for a comprehensive and detailed analysis of histological samples.\n",
    "\n",
    "**Memory and Performance Optimization:** By storing images at different resolutions, computational resources can be optimized by loading only relevant parts of the image based on the required zoom level. This enables efficient handling of images even for large samples.\n",
    "\n",
    "**Extraction of Image Patches:** Pyramidal images facilitate the extraction of image patches at different resolutions. This allows for localized analysis of specific regions of histological samples, which is crucial for many image processing tasks in histopathology.\n",
    "tile_image Function for Creating Image Patches\n",
    "\n",
    "The tile_image function allows for the creation of image patches from pyramidal images with openslide. By subdividing the pyramidal image into patches, this function facilitates data preparation for training machine learning models in histopathology. The resulting patches can be used to train deep neural networks to detect specific features in histological images, thereby contributing to the automation of histopathological analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_image(image_path: str, tile_size: int, output_folder: str, preprocess: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Tiles an image into smaller sub-images and saves each sub-image as a separate file.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        tile_size (int): Size of each tile (both width and height).\n",
    "        output_folder (str): Directory where the tiled images will be saved.\n",
    "        preprocess (bool, optional): Whether to preprocess the tiles before saving. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Open the image using OpenSlide\n",
    "    slide = openslide.OpenSlide(image_path)\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    dimensions = slide.dimensions\n",
    "\n",
    "    # Calculate the number of tiles in each direction\n",
    "    num_tiles_x = int(np.ceil(dimensions[0] / tile_size))\n",
    "    num_tiles_y = int(np.ceil(dimensions[1] / tile_size))\n",
    "\n",
    "    # Slice the image into tiles and save each tile\n",
    "    pbar = tqdm(range(num_tiles_x))\n",
    "    for i in pbar:\n",
    "        for j in range(num_tiles_y):\n",
    "            # Get the coordinates of the tile\n",
    "            x = i * tile_size\n",
    "            y = j * tile_size\n",
    "\n",
    "            # Calculate the size of the tile\n",
    "            w = min(tile_size, dimensions[0] - x)\n",
    "            h = min(tile_size, dimensions[1] - y)\n",
    "\n",
    "            # Read the tile region\n",
    "            tile = slide.read_region((x, y), 0, (w, h))\n",
    "\n",
    "            if preprocess:\n",
    "                # Preprocess the tile (e.g., apply filters, enhance contrast, etc.)\n",
    "                valid_image = preprocess_folder(tile)\n",
    "\n",
    "                if valid_image:\n",
    "                    # Save the preprocessed tile to the output folder\n",
    "                    tile_path = os.path.join(output_folder, f\"tile_{x}_{y}.tiff\")\n",
    "                    tile.save(tile_path)\n",
    "            else:\n",
    "                # Save the tile to the output folder\n",
    "                tile_path = os.path.join(output_folder, f\"tile_{x}_{y}.tiff\")\n",
    "                tile.save(tile_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your dataset with small images.\n",
    "\n",
    "Some images can be read with numpy and theses fuctions can help you patch them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patching(image: np.array, row: int, col: int, verbose: int = 1)->list:\n",
    "    \"\"\"\n",
    "    Divide a image into patches.\n",
    "\n",
    "    Args:\n",
    "        img(np.array): image.\n",
    "        row(int): Number of rows the image will be sliced.\n",
    "        col(int): Number of columns the image will be sliced.\n",
    "        verbose(int): set verbose to '0' if you wish not to have the plot.\n",
    "    Return:\n",
    "        patch(list): Retrun a list of the patches extracted from the image.\n",
    "    \"\"\"\n",
    "\n",
    "    vpatch = np.vsplit(image,row)\n",
    "\n",
    "    hpatch = []\n",
    "    patch = []\n",
    "\n",
    "    for index, vimg in enumerate(vpatch):\n",
    "        hpatch.append(np.hsplit(vimg, col))\n",
    "        len(hpatch)\n",
    "\n",
    "        for himg in hpatch[index]:\n",
    "            patch.append(himg)\n",
    "    if verbose == 1:\n",
    "        for index in range (1, len(patch)+1):\n",
    "\n",
    "            plt.subplot(row, col, index)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(patch[index-1], cmap = 'gray')\n",
    "        plt.show()\n",
    "        print((\"\\n\\n\"))\n",
    "    return np.array(patch)\n",
    "\n",
    "def resize_to_nearest_multiple(image: np.array, target_size=224)-> np.array:\n",
    "    \"\"\"\n",
    "    Resize an image to the nearest multiple of a specified target size.\n",
    "\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image represented as a NumPy array.\n",
    "        target_size (int, optional): Target size for the resizing. Defaults to 224.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized image.\n",
    "    \"\"\"\n",
    "    width, height, _ = image.shape\n",
    "\n",
    "    # Calculate the new width and height to be multiples of the target_size\n",
    "    new_width = int(np.ceil(width / target_size) * target_size)\n",
    "    new_height = int(np.ceil(height / target_size) * target_size)\n",
    "\n",
    "    # Resize the image using OpenCV\n",
    "    resized_img = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized_img\n",
    "\n",
    "def patch_numpy_images(img_path: str, output_folder: str, target_size:int = 224, select_tiles: float = 0.4)->None:\n",
    "    \"\"\"\n",
    "    This function will create patches images of the target size.\n",
    "    Args: \n",
    "        img_path(str): path to the images.\n",
    "        output_folder(str): path to the folder where the patches will be stored.\n",
    "        target_size(int): size of the output tiles.\n",
    "        select_tiles(float): use the preprocess folder to remove the white images  \n",
    "                    coresponding to the background of the slide.\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    target_size = 224\n",
    "    output_folder = f\"{output_folder}/{os.path.splitext(os.path.basename(img_path))[0]}_{target_size}\"\n",
    "\n",
    "    if os.path.exists(output_folder):\n",
    "        print(f\"The folder already exists\")\n",
    "    else: \n",
    "        print(\"Folder doesn't exist, tiling in progress\")\n",
    "\n",
    "        slide = cv2.imread(img_path)\n",
    "        slide = resize_to_nearest_multiple(slide, target_size= target_size)\n",
    "        rows, cols = slide.shape[0]//target_size, slide.shape[1]//target_size\n",
    "        patches = patching(slide, rows, cols)\n",
    "        os.makedirs(output_folder, exist_ok= True)\n",
    "\n",
    "        x,y = 0,0\n",
    "        row, col = 0,0\n",
    "        for tile in tqdm(patches): \n",
    "            if preprocess_folder(tile, select_tiles):\n",
    "                tile_path = os.path.join(output_folder, f\"tile_{x}_{y}.tiff\")\n",
    "                cv2.imwrite(tile_path, tile)\n",
    "\n",
    "                x = target_size*row\n",
    "                y = target_size*col\n",
    "\n",
    "                col+=1\n",
    "                if col%cols == 0:\n",
    "                    row+=1\n",
    "                    col = 0\n",
    "\n",
    "## Uncomment for a use case\n",
    "# img_path = r'Path/to/your/image'\n",
    "# patch_numpy_images(img_path, 'tiles', 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to plot or save images from torch.Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor: torch.Tensor, num_images: int = 16, nrow: int = 4, show: bool = True, output_path: str = None)-> None:\n",
    "    \"\"\"\n",
    "    Function for visualizing images: Given a tensor of images, number of images,\n",
    "    and desired grid layout, plots and displays the images in a uniform grid.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): The input tensor containing images.\n",
    "        num_images (int, optional): Number of images to display. Defaults to 16.\n",
    "        nrow (int, optional): Number of images per row in the grid. Defaults to 4.\n",
    "        show (bool, optional): Whether to display the plot. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "\n",
    "    # Create a grid of images\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "\n",
    "    # Display the grid\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.axis('off')  # Hide axes\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if output_path != None:\n",
    "        save_image(image_grid, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deconv(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels: int = 100, out_channels: int = 64, \n",
    "                 act: nn = None, \n",
    "                 kernel_size: int = 3, \n",
    "                 stride: int = 2, \n",
    "                 dropout: float = 0.4, \n",
    "                 padding: int = 1,\n",
    "                 final_layer: bool = False):\n",
    "        \"\"\"\n",
    "        Deconvolutional layer for a generator block in a DCGAN architecture.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            act (nn.Module): Activation function.\n",
    "            kernel_size (int): Size of the convolutional filter.\n",
    "            stride (int): Stride of the convolution.\n",
    "            dropout (float): Dropout probability.\n",
    "            padding (int): Padding size.\n",
    "            final_layer (bool): True if it is the final layer, False otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Deconv, self).__init__()\n",
    "        self.conv_block = self.block(in_channels, out_channels, act, kernel_size, stride, dropout, padding, final_layer)\n",
    "\n",
    "\n",
    "    def block(self, ic: int, oc: int,\n",
    "            act: nn,\n",
    "            kernel_size: int, \n",
    "            stride: int , \n",
    "            dropout: float, \n",
    "            padding: int,\n",
    "            final_layer: bool):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Args:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            padding (int): Padding size.\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        \n",
    "        Return:\n",
    "            block(nn.Sequential()): return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        '''\n",
    "        if not final_layer:\n",
    "        \n",
    "            block = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(ic, oc, kernel_size, stride, padding),\n",
    "                    nn.BatchNorm2d(oc),\n",
    "                    nn.Dropout(dropout),\n",
    "                    act\n",
    "                    )\n",
    "        \n",
    "        else: \n",
    "            block = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(ic, oc, kernel_size, stride, padding),\n",
    "                    act\n",
    "                    )\n",
    "        \n",
    "        return block\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv_block(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, zdim: int = 100, output_dim: int = 256, num_upsample: int = 3, img_ch: int = 3, activation: nn = nn.ReLU(inplace = True)):\n",
    "\n",
    "        \"\"\"\n",
    "        Generator network for generating images in a DCGAN architecture.\n",
    "\n",
    "        Args:\n",
    "            zdim (int): Dimension of the input noise vector.\n",
    "            output_dim (int): Dimension of the output feature representation.\n",
    "            img_ch (int): Number of channels in the output image.\n",
    "            activation (nn.Module): Activation function.\n",
    "             num_upsample(int): Number of upsampling block, depending on the size of the output image.\n",
    "                            The image size start from 8 and is multiplied by two for each upsampling block?\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.modules_dict = nn.ModuleDict()\n",
    "        self.dim= 8 \n",
    "        self.modules_dict[\"First_layer\"] = nn.Linear(zdim, (output_dim)*self.dim*self.dim)\n",
    "\n",
    "        for index in range(num_upsample):\n",
    "            if index==num_upsample-1:\n",
    "                self.modules_dict[\"last_layer\"] = Deconv(output_dim, img_ch, kernel_size = 4, act = nn.Tanh(), stride = 2, final_layer= True) \n",
    "\n",
    "            else: \n",
    "                self.modules_dict[f\"Deconv_{index}\"] = Deconv(output_dim, output_dim//2, act = activation, kernel_size = 4, stride = 2) \n",
    "            output_dim = output_dim//2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for name, modules in self.modules_dict.items():\n",
    "            x = modules(x)\n",
    "            if name =='First_layer' :\n",
    "                x = x.view(-1, self.output_dim, self.dim, self.dim)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, output_channels: int = 64, img_channels: int = 1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.modules_dict = nn.ModuleDict()\n",
    "        self.modules_dict[\"Block_0\"] = self.block(img_channels, output_channels)\n",
    "        self.modules_dict[\"Block_1\"] = self.block(output_channels, output_channels*2)\n",
    "        self.modules_dict[\"Block_2\"] = self.block(output_channels*2, 1, final_layer = True)\n",
    "\n",
    "    def block(self, \n",
    "            input_channels: int, \n",
    "            output_channels: int, \n",
    "            kernel_size: int = 4, \n",
    "            stride: int = 2, \n",
    "            final_layer: bool = False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Args:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        Return:\n",
    "            block(nn.Sequential()): return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            block =  nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            block =  nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "        \n",
    "        return block\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        for key, modules in self.modules_dict.items():\n",
    "            image = modules(image)\n",
    "        image = image.view(len(image), -1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass()\n",
    "class GANConfigs:\n",
    "    \"\"\"\n",
    "    Configuration class for a Generative Adversarial Network (GAN).\n",
    "\n",
    "    Attributes:\n",
    "        generator (nn.Module): Generator model.\n",
    "        discriminator (nn.Module): Discriminator model.\n",
    "        image_size (int): Size of the input images.\n",
    "        batch_size (int): Batch size.\n",
    "        num_epochs (int): Total number of training epochs.\n",
    "        current_epoch (int): Current epoch number.\n",
    "        display_metrics (int): Interval for displaying metrics during training.\n",
    "        z_dim (int): Dimension of the input noise vector.\n",
    "        gen_lr (float): Learning rate for the generator.\n",
    "        disc_lr (float): Learning rate for the discriminator.\n",
    "        with_lr_scheduler (bool): Whether to use learning rate schedulers.\n",
    "        gen_lr_step_size (int): Step size for the generator's learning rate scheduler.\n",
    "        disc_lr_step_size (int): Step size for the discriminator's learning rate scheduler.\n",
    "        gen_lr_gamma (float): Gamma value for the generator's learning rate scheduler.\n",
    "        disc_lr_gamma (float): Gamma value for the discriminator's learning rate scheduler.\n",
    "        output_dir (str): Directory to save outputs.\n",
    "        curr_epoch (int): Current epoch number (deprecated, use current_epoch instead).\n",
    "        logs (dict): Dictionary to store training logs.\n",
    "        device (str): Device to perform computations ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "    \n",
    "    generator: nn.Module \n",
    "    discriminator: nn.Module \n",
    "\n",
    "    image_size: int = 128\n",
    "    batch_size: int = 32\n",
    "    \n",
    "    num_epochs: int = 100\n",
    "    current_epoch: int = 0\n",
    "    \n",
    "    display_metrics: int = 10\n",
    "\n",
    "    z_dim: int = 100\n",
    "\n",
    "    gen_lr: float = 1e-4 \n",
    "    disc_lr: float = 1e-4\n",
    "\n",
    "    with_lr_scheduler: bool = True\n",
    "\n",
    "    gen_lr_step_size: int = 10\n",
    "    disc_lr_step_size: int = 10\n",
    "\n",
    "    gen_lr_gamma: float = 0.1\n",
    "    disc_lr_gamma: float = 0.1\n",
    "\n",
    "    output_dir: str = None\n",
    "    curr_epoch: int = 0\n",
    "    logs: dict = None\n",
    "    device: str = 'cpu'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes optimizer and learning rate scheduler based on the provided configurations.\n",
    "        \"\"\"\n",
    "\n",
    "        self.gen_optim = torch.optim.Adam(self.generator.parameters(), lr = self.gen_lr)\n",
    "        self.disc_optim = torch.optim.Adam(self.discriminator.parameters(), lr = self.disc_lr)\n",
    "\n",
    "        if self.with_lr_scheduler:\n",
    "            self.gen_lr_schedular = torch.optim.lr_scheduler.StepLR(self.gen_optim, step_size=self.gen_lr_step_size, gamma=self.gen_lr_gamma)\n",
    "            self.disc_lr_schedular = torch.optim.lr_scheduler.StepLR(self.disc_optim, step_size=self.disc_lr_step_size, gamma=self.disc_lr_gamma)\n",
    "\n",
    "        if self.output_dir != None:\n",
    "            os.makedirs(self.output_dir, exist_ok= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WLoss(GANConfigs):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "    def get_gradient(self, real: torch.Tensor, fake: torch.Tensor, epsilon: torch.Tensor):\n",
    "        '''\n",
    "        Return the gradient of the critic's scores with respect to mixes of real and fake images.\n",
    "        \n",
    "        Args:\n",
    "            real (torch.Tensor): A batch of real images.\n",
    "            fake (torch.Tensor): A batch of fake images.\n",
    "            epsilon (torch.Tensor): A vector of the uniformly random proportions of real/fake per mixed image.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The gradient of the critic's scores, with respect to the mixed image.\n",
    "        '''\n",
    "        # Mix the images together\n",
    "        #print(real.shape, fake.shape, epsilon.shape)\n",
    "        mixed_images = real * epsilon + fake * (1 - epsilon)\n",
    "\n",
    "        # Calculate the critic's scores on the mixed images\n",
    "        mixed_scores = self.discriminator(mixed_images)\n",
    "        \n",
    "        # Take the gradient of the scores with respect to the images\n",
    "        gradient = torch.autograd.grad(\n",
    "\n",
    "            inputs=mixed_images,\n",
    "            outputs=mixed_scores,\n",
    "            # These other parameters have to do with the pytorch autograd engine works\n",
    "            grad_outputs=torch.ones_like(mixed_scores), \n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            )[0]\n",
    "        \n",
    "        return gradient\n",
    "\n",
    "    def gradient_penalty(self, gradient: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Return the gradient penalty, given a gradient.\n",
    "        \n",
    "        Given a batch of image gradients, this function calculates the magnitude of each image's gradient\n",
    "        and penalizes the mean quadratic distance of each magnitude to 1.\n",
    "        \n",
    "        Args:\n",
    "            gradient (torch.Tensor): The gradient of the critic's scores, with respect to the mixed image.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: The gradient penalty.\n",
    "        '''\n",
    "        # Flatten the gradients so that each row captures one image\n",
    "        gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "        # Calculate the magnitude of every row\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        \n",
    "        # Penalize the mean squared distance of the gradient norms from 1\n",
    "        penalty = torch.mean(torch.square(gradient_norm-1))\n",
    "        return penalty\n",
    "    \n",
    "\n",
    "    def w_loss_gen(self, crit_fake_pred: torch.Tensor):\n",
    "        '''\n",
    "        Return the loss of a generator given the critic's scores of the generator's fake images.\n",
    "        Args:\n",
    "            crit_fake_pred: the critic's scores of the fake images\n",
    "        Returns:\n",
    "            gen_loss: a scalar loss value for the current batch of the generator\n",
    "        '''\n",
    "        gen_loss = -torch.mean(crit_fake_pred)\n",
    "        return gen_loss\n",
    "    \n",
    "\n",
    "    def w_loss_crit(self, crit_fake_pred: torch.Tensor, crit_real_pred: torch.Tensor, gp: torch.Tensor, c_lambda: int)->torch.Tensor:\n",
    "        '''\n",
    "        Return the loss of a critic given the critic's scores for fake and real images,\n",
    "        the gradient penalty, and gradient penalty weight.\n",
    "        Parameters:\n",
    "            crit_fake_pred(torch.Tensor): the critic's scores of the fake images\n",
    "            crit_real_pred(torch.Tensor): the critic's scores of the real images\n",
    "            gp(torch.Tensor): the unweighted gradient penalty\n",
    "            c_lambda(int): the current weight of the gradient penalty \n",
    "        Returns:\n",
    "            crit_loss(torch.Tensor): a scalar for the critic's loss, accounting for the relevant factors\n",
    "        '''\n",
    "\n",
    "        wasserstein_estimate = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred)\n",
    "        \n",
    "        # Compute the gradient penalty term\n",
    "        crit_loss = wasserstein_estimate + c_lambda * gp\n",
    "\n",
    "        return crit_loss\n",
    "    \n",
    "    def update_critic(self, \n",
    "                    real: torch.tensor = None, \n",
    "                    critic_repeats: int = 5, \n",
    "                    c_lambda: int = 10,\n",
    "                    )->int:\n",
    "        \"\"\"\n",
    "        Update the critic (discriminator) network for a specified number of iterations.\n",
    "\n",
    "        Args:\n",
    "            self (object): The instance of the class containing the generator, critic, and optimizer.\n",
    "            real (torch.tensor, optional): The real input data for the critic. Defaults to None.\n",
    "            critic_repeats (int, optional): The number of iterations to update the critic. Defaults to 5.\n",
    "            c_lambda (int, optional): The coefficient for the gradient penalty term. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "            list: A list containing the average critic loss for this iteration.\n",
    "        \"\"\"\n",
    "        n_samples = len(real)\n",
    "        mean_iteration_critic_loss = 0\n",
    "        for _ in range(critic_repeats):\n",
    "\n",
    "            self.disc_optim.zero_grad()\n",
    "            fake_noise = torch.randn(n_samples, self.z_dim, device = self.device)\n",
    "            fake = self.generator(fake_noise)\n",
    "\n",
    "            crit_fake_pred = self.discriminator(fake.detach())\n",
    "            crit_real_pred = self.discriminator(real)\n",
    "\n",
    "            epsilon = torch.rand(n_samples, 1, 1, 1, device=self.device, requires_grad=True)\n",
    "            gradient = self.get_gradient(real, fake.detach(), epsilon)\n",
    "            gp = self.gradient_penalty(gradient)\n",
    "            crit_loss = self.w_loss_crit(crit_fake_pred, crit_real_pred, gp, c_lambda)\n",
    "\n",
    "            # Keep track of the average critic loss in this batch\n",
    "            mean_iteration_critic_loss += crit_loss.item() / critic_repeats\n",
    "            # Update gradients\n",
    "            crit_loss.backward(retain_graph=True)\n",
    "            # Update optimizer\n",
    "            self.disc_optim.step()\n",
    "        \n",
    "        return [mean_iteration_critic_loss]\n",
    "    \n",
    "    def update_w_gen(self, n_samples: int = None) -> torch.Tensor:\n",
    "        '''\n",
    "        Update the generator network parameters based on the current critic's scores of fake images.\n",
    "        \n",
    "        Args:\n",
    "            n_samples (int): Number of samples to generate and update with.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: A list containing the generator loss for the current batch.\n",
    "        '''\n",
    "        self.gen_optim.zero_grad()\n",
    "        fake_noise = torch.randn(n_samples, self.z_dim, device=self.device)\n",
    "        fake = self.generator(fake_noise)\n",
    "        crit_fake_pred = self.discriminator(fake)\n",
    "        \n",
    "        gen_loss = self.w_loss_gen(crit_fake_pred)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        self.gen_optim.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        return [gen_loss.item()]\n",
    "\n",
    "    def show_metrics(self, real: torch.tensor):\n",
    "        '''\n",
    "        Show training metrics such as generator and discriminator losses and visualize generated images.\n",
    "        \n",
    "        Args:\n",
    "            real (torch.Tensor): A batch of real images.\n",
    "        '''\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        self.gen_optim.zero_grad()\n",
    "        fake_noise = torch.randn(8, self.z_dim, device=self.device)\n",
    "        fake = self.generator(fake_noise)\n",
    "        image_path = f'epoch_{self.curr_epoch}.jpg'\n",
    "        output_image = os.path.join(self.output_dir,image_path)\n",
    "        show_tensor_images(fake, num_images = 8, output_path = output_image)\n",
    "        show_tensor_images(real, num_images = 8)\n",
    "        \n",
    "        plt.plot(self.logs[\"gen_loss_on_epoch\"], label=\"Generator Loss\", color=\"blue\")\n",
    "        plt.plot(self.logs[\"disc_loss_on_epoch\"], label=\"Discriminator Loss\", color=\"red\")\n",
    "        plt.xlabel(f\"epoch: {self.curr_epoch}\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Losses\")\n",
    "        plt.legend()\n",
    "        plt.show()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_WGAN(configs: None= None, dataloader: torch.utils.data = None, c_lambda = 10)->None:       \n",
    "    generator_losses = []\n",
    "    discriminator_losses = []\n",
    "\n",
    "    gen_loss_on_epoch = []\n",
    "    disc_loss_on_epoch = []\n",
    "    for epoch in range(configs.num_epochs):\n",
    "\n",
    "        # Dataloader returns the batches of real images\n",
    "        pbar = tqdm(total = len(dataloader))\n",
    "        pbar.set_description(f\"Epoch: {epoch}\")\n",
    "\n",
    "        for real, _ in dataloader:\n",
    "\n",
    "            cur_batch_size = len(real)\n",
    "            # Flatten the batch of real images from the dataset\n",
    "            real = real.to(configs.device)\n",
    "\n",
    "            discriminator_losses += configs.update_critic(real, c_lambda = c_lambda)\n",
    "            generator_losses += configs.update_w_gen(n_samples = cur_batch_size)\n",
    "\n",
    "            # Store generator loss\n",
    "            generator_losses.append(generator_losses[-1])\n",
    "            discriminator_losses.append(discriminator_losses[-1])\n",
    "            configs.logs = {\n",
    "                    \"gen_loss\": generator_losses[-1], \n",
    "                    \"disc_loss\": discriminator_losses[-1],\n",
    "                    \"gen_lr\": configs.gen_lr_schedular.get_last_lr()[0], \n",
    "                    \"disc_lr\": configs.disc_lr_schedular.get_last_lr()[0],\n",
    "                    }\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(**configs.logs)\n",
    "        \n",
    "        gen_loss_on_epoch.append(generator_losses[-1])\n",
    "        disc_loss_on_epoch.append(discriminator_losses[-1])\n",
    "\n",
    "        configs.logs[\"gen_loss_on_epoch\"] = gen_loss_on_epoch\n",
    "        configs.logs[\"disc_loss_on_epoch\"] = disc_loss_on_epoch\n",
    "        \n",
    "        if (epoch % configs.display_metrics == 0) or (epoch == configs.num_epochs-1):\n",
    "            configs.show_metrics(real)\n",
    "\n",
    "            # Save models each display_metrics\n",
    "            torch.save(configs.generator.state_dict(), \"gen_model.pth\")\n",
    "            torch.save(configs.discriminator.state_dict(), \"disc_model.pth\")\n",
    "        \n",
    "        if configs.with_lr_scheduler:\n",
    "\n",
    "            configs.gen_lr_schedular.step()\n",
    "            configs.disc_lr_schedular.step()\n",
    "        \n",
    "\n",
    "        configs.curr_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (3, 64, 64)\n",
    "z_dim = 200\n",
    "\n",
    "gen = Generator(z_dim, output_dim = 128, img_ch = img_shape[0], num_upsample = 3).to(device)\n",
    "disc = Discriminator(output_channels= 128, img_channels = img_shape[0]).to(device)\n",
    "\n",
    "configs = WLoss(gen, disc, z_dim= z_dim,\n",
    "                gen_lr_gamma = 0.5,\n",
    "                gen_lr_step_size = 15,\n",
    "\n",
    "                disc_lr_gamma = 0.5,\n",
    "                disc_lr_step_size=15,\n",
    "                image_size = img_shape[1], \n",
    "                batch_size = 32, \n",
    "                num_epochs = 100, \n",
    "                display_metrics = 1, \n",
    "                device = device, \n",
    "                output_dir = \"gen_images\"\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, we devide or dataset into training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "folder_path = r\"your/dataset/path\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((configs.image_size, configs.image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "dataset = ImageFolder(folder_path, transform=transform)\n",
    "\n",
    "# Calculer la taille du sous-ensemble désiré (50%)\n",
    "subset_size = int(0.1 * len(dataset))\n",
    "\n",
    "# Générer des indices aléatoires pour le sous-ensemble\n",
    "indices = torch.randperm(len(dataset))\n",
    "\n",
    "# Sélectionner les indices du sous-ensemble\n",
    "train_set = indices[:subset_size]\n",
    "test_set = indices[subset_size:]\n",
    "# Créer un sampler à partir des indices du sous-ensemble\n",
    "train_subset_sampler = SubsetRandomSampler(train_set)\n",
    "test_subset_sampler = SubsetRandomSampler(train_set)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size= configs.batch_size, sampler = train_subset_sampler)\n",
    "test_loader = DataLoader(dataset, batch_size= configs.batch_size, sampler = test_subset_sampler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only pass the test_set trough the training because the training set is to big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_WGAN(configs, test_loader, c_lambda = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(configs.generator.state_dict(), \"gen_model.pth\")\n",
    "torch.save(configs.discriminator.state_dict(), \"disc_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def create_gif_from_images(folder_path: str, output_gif_path: str, duration: float = 100.)->None:\n",
    "    \"\"\"\n",
    "    Create a GIF from images in a folder and save it to the specified output path.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the images.\n",
    "        output_gif_path (str): Path to save the output GIF.\n",
    "        duration (float): Duration (in seconds) between each image in the GIF (default is 0.5 seconds).\n",
    "    \"\"\"\n",
    "    # List of image files in the folder\n",
    "    image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.jpg')]\n",
    "    \n",
    "    # List of images\n",
    "    images = [imageio.imread(file) for file in image_files]\n",
    "    \n",
    "    # Write the GIF\n",
    "    imageio.mimsave(output_gif_path, images, duration=duration)\n",
    "\n",
    "create_gif_from_images(configs.output_dir, 'res_gif_64.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
